{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwi9mzFPPDv7"
   },
   "source": [
    "# Identifying Entities in Healthcare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "T4hxafMCE1nn"
   },
   "outputs": [],
   "source": [
    "# Importing Neccessary Libraries library\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bz6czVx9R6WJ"
   },
   "outputs": [],
   "source": [
    "# Reading in the Data \n",
    "with open('train_sent', 'r') as train_sent_file:\n",
    "  train_sentences = train_sent_file.readlines() #====> train sentences \n",
    "\n",
    "with open('train_label', 'r') as train_labels_file:\n",
    "  train_labels = train_labels_file.readlines() #====> train label\n",
    "\n",
    "with open('test_sent', 'r') as test_sent_file:\n",
    "  test_sentences = test_sent_file.readlines() #====> test sentences \n",
    "\n",
    "with open('test_label', 'r') as test_labels_file:\n",
    "  test_labels = test_labels_file.readlines() #====> test Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence and labels construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining patterns so that we can convert the given format into sentences using the rules giving for sentence extraction\n",
    "\n",
    "patterns=\"^\\n\" \n",
    "pattern2=\"[^\\n]*\"\n",
    "\n",
    "# Function to convert our raw input to sentences\n",
    "\n",
    "def makesentence(text,kind): #====>Kind to specify 'ifis label' so no space is added between the character\n",
    "    line=\"\"\n",
    "    sentence=[]\n",
    "    for word in text:\n",
    "        if re.match(patterns,word):\n",
    "            sentence.append(line)\n",
    "            line=\"\"     \n",
    "        else:\n",
    "            newword=re.search(pattern2,word)\n",
    "            if (line==\"\" or kind==\"label\"):\n",
    "                line=line+newword.group(0)\n",
    "            else:\n",
    "                line=line+\" \"+newword.group(0)\n",
    "    return sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the makesentence function to construct the sentences and label in the format we need\n",
    "train_sent=makesentence(train_sentences,\"sent\") #---> Train Sentences from words\n",
    "train_label=makesentence(train_labels,\"label\")  #---> Train Labels\n",
    "test_sent=makesentence(test_sentences,\"sent\")   #---> Test Sentences from Words\n",
    "test_label=makesentence(test_labels,\"label\")    #---> Test Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of sentences in the processed train and test dataset and Count the number of lines of labels in the processed train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set===>\n",
      "Number of Sentence in train set : 2599\n",
      "Number of Label rows in train set : 2599\n",
      "Test Set===>\n",
      "Number of Sentence in test set : 1056\n",
      "Number of Label rows in train set : 1056\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set===>\")\n",
    "print(\"Number of Sentence in train set :\",len(train_sent))\n",
    "print(\"Number of Label rows in train set :\",len(train_label))\n",
    "print(\"Test Set===>\")\n",
    "print(\"Number of Sentence in test set :\",len(test_sent))\n",
    "print(\"Number of Label rows in train set :\",len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 2599 sentences with 2599 rows of labels for train dataset and 1056 sentences and 1056 labels for test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing 5 Sentences from Training Set along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence : All live births > or = 23 weeks at the University of Vermont in 1995 ( n = 2395 ) were retrospectively analyzed for delivery route , indication for cesarean , gestational age , parity , and practice group ( to reflect risk status )\n",
      "Label : OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
      "Number of Words in the sentences are :  45\n",
      "Number of labels for this sentence is :  45\n",
      "-------------------------------------------\n",
      "Sentence : RESULTS : Forty-four patients in the partner intrauterine insemination group and 37 in the donor insemination group were identified as having primary infertility\n",
      "Label : OOOOOOOTTOOOOOTTOOOOOOD\n",
      "Number of Words in the sentences are :  23\n",
      "Number of labels for this sentence is :  23\n",
      "-------------------------------------------\n",
      "Sentence : The effect of epinephrine on immunoreactive insulin levels in man\n",
      "Label : OOOOOOOOOO\n",
      "Number of Words in the sentences are :  10\n",
      "Number of labels for this sentence is :  10\n",
      "-------------------------------------------\n",
      "Sentence : The prison patient\n",
      "Label : OOO\n",
      "Number of Words in the sentences are :  3\n",
      "Number of labels for this sentence is :  3\n",
      "-------------------------------------------\n",
      "Sentence : Effect of protein kinase C inhibitors on cardioprotection by ischemic preconditioning depends on the number of preconditioning episodes\n",
      "Label : OOOOOOOOOOOOOOOOOO\n",
      "Number of Words in the sentences are :  18\n",
      "Number of labels for this sentence is :  18\n",
      "-------------------------------------------\n",
      "Sentence : The protective effect of condoms and nonoxynol-9 against HIV infection\n",
      "Label : OOOOTOTODD\n",
      "Number of Words in the sentences are :  10\n",
      "Number of labels for this sentence is :  10\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We will print out the sentence along with it's corresponding labels and the length of both. \n",
    "# A mismatch in len should indicate data extraction error\n",
    "\n",
    "for i in range(0,2599,500): #----> Gap of 500 so we can sample the data across the set\n",
    "    print(\"Sentence :\",train_sent[i])\n",
    "    print(\"Label :\",train_label[i])\n",
    "    print(\"Number of Words in the sentences are : \",len(train_sent[i].split()))\n",
    "    print(\"Number of labels for this sentence is : \",len(train_label[i]))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing 5 Sentences from Test Set along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence : Furthermore , when all deliveries were analyzed , regardless of risk status but limited to gestational age > or = 36 weeks , the rates did not change ( 12.6 % , 280 of 2214 ; primary 9.2 % , 183 of 1994 )\n",
      "Label : OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
      "Number of Words in the sentences are :  44\n",
      "Number of labels for this sentence is :  44\n",
      "-------------------------------------------\n",
      "Sentence : The study was terminated at 30 min if satisfactory analgesia was not achieved\n",
      "Label : OOOOOOOOOOOOO\n",
      "Number of Words in the sentences are :  13\n",
      "Number of labels for this sentence is :  13\n",
      "-------------------------------------------\n",
      "Sentence : The measuring function of the first legs of Araneus diadematus Cl\n",
      "Label : OOOOOOOOOOO\n",
      "Number of Words in the sentences are :  11\n",
      "Number of labels for this sentence is :  11\n",
      "-------------------------------------------\n",
      "Sentence : Drugs for the Third World\n",
      "Label : OOOOO\n",
      "Number of Words in the sentences are :  5\n",
      "Number of labels for this sentence is :  5\n",
      "-------------------------------------------\n",
      "Sentence : 121\n",
      "Label : O\n",
      "Number of Words in the sentences are :  1\n",
      "Number of labels for this sentence is :  1\n",
      "-------------------------------------------\n",
      "Sentence : Antimicrobial treatment options in the management of odontogenic infections\n",
      "Label : TTOOOOODD\n",
      "Number of Words in the sentences are :  9\n",
      "Number of labels for this sentence is :  9\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We will print out the sentence along with it's corresponding labels and the length of both. \n",
    "# A mismatch in len should indicate data extraction error\n",
    "\n",
    "for i in range(0,1056,200): #----> Gap of 200 so we can sample the data across the set\n",
    "    print(\"Sentence :\",test_sent[i])\n",
    "    print(\"Label :\",test_label[i])\n",
    "    print(\"Number of Words in the sentences are : \",len(test_sent[i].split()))\n",
    "    print(\"Number of labels for this sentence is : \",len(test_label[i]))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract those tokens which have NOUN or PROPN as their PoS tag and find their frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining train and test sets into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Sentence in the full dataset : 3655\n",
      "Total Number of labels rows in the full dataset : 3655\n"
     ]
    }
   ],
   "source": [
    "# Combining train and test into one\n",
    "full_sent=train_sent+test_sent\n",
    "full_label=train_label+test_label\n",
    "print(\"Total Number of Sentence in the full dataset :\",len(full_sent))\n",
    "print(\"Total Number of labels rows in the full dataset :\",len(full_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sentences with mismatched labels found\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if all labels are the same length as their respective sentences\n",
    "error=0\n",
    "for i in range(0,3655):\n",
    "    if(len(full_sent[i].split())!=len(full_label[i])):\n",
    "        error=error+1\n",
    "if error==0:\n",
    "    print(\"No sentences with mismatched labels found\")\n",
    "else:\n",
    "    print(\"Number of Sentences with mismatched labels are :\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for words marked as nouns and proper nouns\n",
    "##### We will check once with whole set as it was given once  and once again with stopword removed so we will print 4 set of frequency\n",
    "###### PLEASE NOTE:  Stop word removal is just for checking full dataset. It will not be implemented on either the train or test data set for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Spacy parameters\n",
    "model = spacy.load(\"en_core_web_sm\",disable=['parser','ner','lemmatizer','textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Function to count and display information for nouns and propn from the passed list\n",
    "def countanddisplay(full_sent):\n",
    "    NOUN=[] #-- Empty list to store words marked as nouns\n",
    "    PROPN=[] # -- Empty list to store words marked as propn\n",
    "    totalwords=0 #-- Counter to count number of words in the dataset\n",
    "    for sent in full_sent:\n",
    "        doc=model(sent)\n",
    "        for tok in doc:\n",
    "            totalwords += 1\n",
    "            if tok.pos_==\"NOUN\":\n",
    "                NOUN.append(tok.text.lower())\n",
    "            if tok.pos_==\"PROPN\":\n",
    "                PROPN.append(tok.text.lower())\n",
    "    #Frequency of Nouns & PROPN\n",
    "    print(\"--------------Noun--------------\")\n",
    "    print(\"The number of word marked as nouns is :\",len(NOUN)) #-- Number of Nouns\n",
    "    print(\"The number of unique nouns is :\",len(set(NOUN)))    #-- Number of Unique Nouns\n",
    "    print(\"The Percentage of words marked as nouns is  :\",len(NOUN)/totalwords*100,\"%\") #-percentage of Nouns over the full dataset\n",
    "    print(\"--------------PRONP--------------\")\n",
    "    print(\"The number of word marked as proper noun is :\",len(PROPN)) #-- Number of P-Nouns\n",
    "    print(\"The number of unique proper noun is :\",len(set(PROPN))) #-- Number of unique P-Nouns\n",
    "    print(\"The Percentage of words marked as proper noun is  :\",len(PROPN)/totalwords*100,\"%\\n\") #-Percentage og P-noun over full dataset\n",
    "    # Top 25 words marked as Nouns\n",
    "    print(\"Top 25 words marked as Nouns\")\n",
    "    NOUN=pd.Series(NOUN)\n",
    "    print(NOUN.value_counts().head(25),\"\\n\")\n",
    "    # Top 25 words marked as Proper Nouns\n",
    "    print(\"Top 25 words marked as Proper Nouns\")\n",
    "    PROPN=pd.Series(PROPN)\n",
    "    print(PROPN.value_counts().head(25))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Full Dataset with no stopword removal---------\n",
      "--------------Noun--------------\n",
      "The number of word marked as nouns is : 20637\n",
      "The number of unique nouns is : 4465\n",
      "The Percentage of words marked as nouns is  : 30.39009233216016 %\n",
      "--------------PRONP--------------\n",
      "The number of word marked as proper noun is : 3739\n",
      "The number of unique proper noun is : 1756\n",
      "The Percentage of words marked as proper noun is  : 5.506059758198713 %\n",
      "\n",
      "Top 25 words marked as Nouns\n",
      "patients        507\n",
      "treatment       303\n",
      "%               247\n",
      "cancer          204\n",
      "therapy         177\n",
      "study           161\n",
      "disease         142\n",
      "cell            141\n",
      "lung            117\n",
      "results         117\n",
      "effects          99\n",
      "group            94\n",
      "gene             91\n",
      "chemotherapy     89\n",
      "effect           82\n",
      "women            81\n",
      "analysis         76\n",
      "use              75\n",
      "risk             74\n",
      "surgery          73\n",
      "cases            72\n",
      "rate             68\n",
      "response         66\n",
      "children         65\n",
      "survival         65\n",
      "dtype: int64 \n",
      "\n",
      "Top 25 words marked as Proper Nouns\n",
      "to_see           74\n",
      "c                32\n",
      "hiv              29\n",
      "methods          26\n",
      "b                23\n",
      "a                22\n",
      "ii               21\n",
      "nsclc            19\n",
      "preeclampsia     19\n",
      "s                19\n",
      "interferon       19\n",
      "g                17\n",
      "group            17\n",
      "conclusion       17\n",
      "co2              17\n",
      "aids             14\n",
      "study            14\n",
      "hodgkin          14\n",
      "mg               14\n",
      "-                14\n",
      "american         14\n",
      "international    14\n",
      "l.               13\n",
      "csf              13\n",
      "use              13\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"---------Full Dataset with no stopword removal---------\")\n",
    "countanddisplay(full_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking with stopword removed\n",
    "### Again note: Only for exploration, stopword will not be removed for training or test during modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopword(list_sent):\n",
    "    for sent in list_sent:\n",
    "        sent_word = sent.split()\n",
    "        sent_word = [word for word in sent_word if word not in stopwords.words('english')]\n",
    "        sent = \" \".join(sent_word)\n",
    "        stop_sent.append(sent)\n",
    "    return(stop_sent)\n",
    "\n",
    "stop_sent=[]\n",
    "stopped=stopword(full_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Full Dataset with  stopword removal---------\n",
      "--------------Noun--------------\n",
      "The number of word marked as nouns is : 19755\n",
      "The number of unique nouns is : 4382\n",
      "The Percentage of words marked as nouns is  : 40.0004049648693 %\n",
      "--------------PRONP--------------\n",
      "The number of word marked as proper noun is : 4085\n",
      "The number of unique proper noun is : 1977\n",
      "The Percentage of words marked as proper noun is  : 8.271407455403244 %\n",
      "\n",
      "Top 25 words marked as Nouns\n",
      "patients        502\n",
      "treatment       299\n",
      "%               247\n",
      "cancer          203\n",
      "therapy         177\n",
      "study           156\n",
      "disease         142\n",
      "cell            140\n",
      "lung            115\n",
      "results         107\n",
      "effects          98\n",
      "gene             91\n",
      "group            90\n",
      "chemotherapy     89\n",
      "women            81\n",
      "analysis         76\n",
      "effect           74\n",
      "risk             73\n",
      "surgery          73\n",
      "rate             68\n",
      "cases            66\n",
      "children         65\n",
      "response         65\n",
      "survival         65\n",
      "p                62\n",
      "dtype: int64 \n",
      "\n",
      "Top 25 words marked as Proper Nouns\n",
      "to_see           74\n",
      "c                31\n",
      "hiv              29\n",
      "ii               26\n",
      "methods          26\n",
      "b                24\n",
      "group            21\n",
      "s                20\n",
      "preeclampsia     20\n",
      "a                19\n",
      "conclusion       17\n",
      "interferon       17\n",
      "g                16\n",
      "study            16\n",
      "co2              16\n",
      "csf              14\n",
      "international    14\n",
      "aids             14\n",
      "american         14\n",
      "hodgkin          14\n",
      "-                14\n",
      "ml               13\n",
      "nsclc            13\n",
      "l.               13\n",
      "dna              13\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"---------Full Dataset with  stopword removal---------\")\n",
    "countanddisplay(stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion : As expected stopword removal did not really affect the nouns and propernouns as they most likely would not \n",
    "# be removed using stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE - Model Building Steps\n",
    "#### Get POS Tags\n",
    "#### Create CRF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to take full list of sentence and than break them into sentences and than break those sentences into words\n",
    "# and add pos tags to the words using nltk\n",
    "def pos_tags(sentences):\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "        return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag  both train and test dataset\n",
    "train_sent_tagged=pos_tags(train_sent) #-- Get tagged train data\n",
    "test_sent_tagged=pos_tags(test_sent) #-- Get tagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('All', 'DT'),\n",
       " ('live', 'JJ'),\n",
       " ('births', 'NNS'),\n",
       " ('>', 'VBP'),\n",
       " ('or', 'CC'),\n",
       " ('=', 'VBP'),\n",
       " ('23', 'CD'),\n",
       " ('weeks', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('University', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Vermont', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1995', 'CD'),\n",
       " ('(', '('),\n",
       " ('n', 'IN'),\n",
       " ('=', 'NNP'),\n",
       " ('2395', 'CD'),\n",
       " (')', ')'),\n",
       " ('were', 'VBD'),\n",
       " ('retrospectively', 'RB'),\n",
       " ('analyzed', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('delivery', 'NN'),\n",
       " ('route', 'NN'),\n",
       " (',', ','),\n",
       " ('indication', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('cesarean', 'NN'),\n",
       " (',', ','),\n",
       " ('gestational', 'JJ'),\n",
       " ('age', 'NN'),\n",
       " (',', ','),\n",
       " ('parity', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('practice', 'NN'),\n",
       " ('group', 'NN'),\n",
       " ('(', '('),\n",
       " ('to', 'TO'),\n",
       " ('reflect', 'VB'),\n",
       " ('risk', 'NN'),\n",
       " ('status', 'NN'),\n",
       " (')', ')')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output of pos_tags function\n",
    "#For train dataset\n",
    "train_sent_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the sentences are being returned with words marked with their POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furthermore', 'RB'),\n",
       " (',', ','),\n",
       " ('when', 'WRB'),\n",
       " ('all', 'DT'),\n",
       " ('deliveries', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('analyzed', 'VBN'),\n",
       " (',', ','),\n",
       " ('regardless', 'RB'),\n",
       " ('of', 'IN'),\n",
       " ('risk', 'NN'),\n",
       " ('status', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('limited', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('gestational', 'JJ'),\n",
       " ('age', 'NN'),\n",
       " ('>', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('=', '$'),\n",
       " ('36', 'CD'),\n",
       " ('weeks', 'NNS'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('rates', 'NNS'),\n",
       " ('did', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('change', 'NN'),\n",
       " ('(', '('),\n",
       " ('12.6', 'CD'),\n",
       " ('%', 'NN'),\n",
       " (',', ','),\n",
       " ('280', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('2214', 'CD'),\n",
       " (';', ':'),\n",
       " ('primary', 'JJ'),\n",
       " ('9.2', 'CD'),\n",
       " ('%', 'NN'),\n",
       " (',', ','),\n",
       " ('183', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('1994', 'CD'),\n",
       " (')', ')')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For test dataset\n",
    "test_sent_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the sentences are being returned with words marked with their POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to create features for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ==> sklearn-crfsuite Documentation release 0.3 by Mikhail Korobov\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0] # --- The word\n",
    "    postag = sent[i][1] # --- The postag of the world\n",
    "    \n",
    "    features = {\n",
    "        'bias': 0.75, #-- Expermimented with 1, 0.9,0,8,0.75 and 0.7 and 0.75 gave the highest f1 score\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "        \n",
    "    }\n",
    "    if i > 0: #--------> Previous Word \n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True #-----> If begining of sentence\n",
    "        \n",
    "    if i < len(sent)-1: #--------> Next Word\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:word.isdigit()': word1.isdigit(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True #-----> If end of sentence\n",
    "                \n",
    "    return features\n",
    "\n",
    "### - Sends the sentence with the index of the word which has to be processed to the word2features function\n",
    "def sent2features(sent): \n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split labels as list===>\n",
    "def getLabel(labels):\n",
    "    return [label for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling sent2feature function on the pos tagged data to create features for CRF\n",
    "# Calling getlabels to create label in the required format\n",
    "\n",
    "X_train=[sent2features(s) for s in train_sent_tagged] #---- Train Data -X\n",
    "Y_train=[getLabel(label) for label in train_label]   #---- Train Data -Y\n",
    "X_test=[sent2features(s) for s in test_sent_tagged] #---- Test Data -X\n",
    "Y_test=[getLabel(label) for label in test_label]   #---- Test Data -Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 0.75,\n",
       "  'word.lower()': 'all',\n",
       "  'word[-3:]': 'All',\n",
       "  'word[-2:]': 'll',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'DT',\n",
       "  'postag[:2]': 'DT',\n",
       "  'BOS': True,\n",
       "  '+1:word.lower()': 'live',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'JJ',\n",
       "  '+1:postag[:2]': 'JJ'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'live',\n",
       "  'word[-3:]': 'ive',\n",
       "  'word[-2:]': 've',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'JJ',\n",
       "  'postag[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'all',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'DT',\n",
       "  '-1:postag[:2]': 'DT',\n",
       "  '+1:word.lower()': 'births',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NNS',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'births',\n",
       "  'word[-3:]': 'ths',\n",
       "  'word[-2:]': 'hs',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNS',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'live',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'JJ',\n",
       "  '-1:postag[:2]': 'JJ',\n",
       "  '+1:word.lower()': '>',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'VBP',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '>',\n",
       "  'word[-3:]': '>',\n",
       "  'word[-2:]': '>',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBP',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'births',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NNS',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'or',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'CC',\n",
       "  '+1:postag[:2]': 'CC'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'or',\n",
       "  'word[-3:]': 'or',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'CC',\n",
       "  'postag[:2]': 'CC',\n",
       "  '-1:word.lower()': '>',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'VBP',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': '=',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'VBP',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '=',\n",
       "  'word[-3:]': '=',\n",
       "  'word[-2:]': '=',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBP',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'or',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'CC',\n",
       "  '-1:postag[:2]': 'CC',\n",
       "  '+1:word.lower()': '23',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': True,\n",
       "  '+1:postag': 'CD',\n",
       "  '+1:postag[:2]': 'CD'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '23',\n",
       "  'word[-3:]': '23',\n",
       "  'word[-2:]': '23',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'CD',\n",
       "  'postag[:2]': 'CD',\n",
       "  '-1:word.lower()': '=',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'VBP',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'weeks',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NNS',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'weeks',\n",
       "  'word[-3:]': 'eks',\n",
       "  'word[-2:]': 'ks',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNS',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': '23',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': True,\n",
       "  '-1:postag': 'CD',\n",
       "  '-1:postag[:2]': 'CD',\n",
       "  '+1:word.lower()': 'at',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'at',\n",
       "  'word[-3:]': 'at',\n",
       "  'word[-2:]': 'at',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'weeks',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NNS',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'DT',\n",
       "  '+1:postag[:2]': 'DT'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'DT',\n",
       "  'postag[:2]': 'DT',\n",
       "  '-1:word.lower()': 'at',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'university',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NNP',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'university',\n",
       "  'word[-3:]': 'ity',\n",
       "  'word[-2:]': 'ty',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNP',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'DT',\n",
       "  '-1:postag[:2]': 'DT',\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'university',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NNP',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'vermont',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NNP',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'vermont',\n",
       "  'word[-3:]': 'ont',\n",
       "  'word[-2:]': 'nt',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNP',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'in',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'in',\n",
       "  'word[-3:]': 'in',\n",
       "  'word[-2:]': 'in',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'vermont',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NNP',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': '1995',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': True,\n",
       "  '+1:postag': 'CD',\n",
       "  '+1:postag[:2]': 'CD'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '1995',\n",
       "  'word[-3:]': '995',\n",
       "  'word[-2:]': '95',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'CD',\n",
       "  'postag[:2]': 'CD',\n",
       "  '-1:word.lower()': 'in',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': '(',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': '(',\n",
       "  '+1:postag[:2]': '('},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '(',\n",
       "  'word[-3:]': '(',\n",
       "  'word[-2:]': '(',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': '(',\n",
       "  'postag[:2]': '(',\n",
       "  '-1:word.lower()': '1995',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': True,\n",
       "  '-1:postag': 'CD',\n",
       "  '-1:postag[:2]': 'CD',\n",
       "  '+1:word.lower()': 'n',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'n',\n",
       "  'word[-3:]': 'n',\n",
       "  'word[-2:]': 'n',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': '(',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': '(',\n",
       "  '-1:postag[:2]': '(',\n",
       "  '+1:word.lower()': '=',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NNP',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '=',\n",
       "  'word[-3:]': '=',\n",
       "  'word[-2:]': '=',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NNP',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'n',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': '2395',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': True,\n",
       "  '+1:postag': 'CD',\n",
       "  '+1:postag[:2]': 'CD'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '2395',\n",
       "  'word[-3:]': '395',\n",
       "  'word[-2:]': '95',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'postag': 'CD',\n",
       "  'postag[:2]': 'CD',\n",
       "  '-1:word.lower()': '=',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NNP',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': ')',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': ')',\n",
       "  '+1:postag[:2]': ')'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': ')',\n",
       "  'word[-3:]': ')',\n",
       "  'word[-2:]': ')',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': ')',\n",
       "  'postag[:2]': ')',\n",
       "  '-1:word.lower()': '2395',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': True,\n",
       "  '-1:postag': 'CD',\n",
       "  '-1:postag[:2]': 'CD',\n",
       "  '+1:word.lower()': 'were',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'VBD',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'were',\n",
       "  'word[-3:]': 'ere',\n",
       "  'word[-2:]': 're',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBD',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': ')',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': ')',\n",
       "  '-1:postag[:2]': ')',\n",
       "  '+1:word.lower()': 'retrospectively',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'RB',\n",
       "  '+1:postag[:2]': 'RB'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'retrospectively',\n",
       "  'word[-3:]': 'ely',\n",
       "  'word[-2:]': 'ly',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'RB',\n",
       "  'postag[:2]': 'RB',\n",
       "  '-1:word.lower()': 'were',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'VBD',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'analyzed',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'VBN',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'analyzed',\n",
       "  'word[-3:]': 'zed',\n",
       "  'word[-2:]': 'ed',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VBN',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'retrospectively',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'RB',\n",
       "  '-1:postag[:2]': 'RB',\n",
       "  '+1:word.lower()': 'for',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'for',\n",
       "  'word[-3:]': 'for',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'analyzed',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'VBN',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'delivery',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'delivery',\n",
       "  'word[-3:]': 'ery',\n",
       "  'word[-2:]': 'ry',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'for',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': 'route',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'route',\n",
       "  'word[-3:]': 'ute',\n",
       "  'word[-2:]': 'te',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'delivery',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': ',',\n",
       "  '+1:postag[:2]': ','},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': ',',\n",
       "  'postag[:2]': ',',\n",
       "  '-1:word.lower()': 'route',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'indication',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'indication',\n",
       "  'word[-3:]': 'ion',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': ',',\n",
       "  '-1:postag[:2]': ',',\n",
       "  '+1:word.lower()': 'for',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'IN',\n",
       "  '+1:postag[:2]': 'IN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'for',\n",
       "  'word[-3:]': 'for',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'IN',\n",
       "  'postag[:2]': 'IN',\n",
       "  '-1:word.lower()': 'indication',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'cesarean',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'cesarean',\n",
       "  'word[-3:]': 'ean',\n",
       "  'word[-2:]': 'an',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'for',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'IN',\n",
       "  '-1:postag[:2]': 'IN',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': ',',\n",
       "  '+1:postag[:2]': ','},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': ',',\n",
       "  'postag[:2]': ',',\n",
       "  '-1:word.lower()': 'cesarean',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'gestational',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'JJ',\n",
       "  '+1:postag[:2]': 'JJ'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'gestational',\n",
       "  'word[-3:]': 'nal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'JJ',\n",
       "  'postag[:2]': 'JJ',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': ',',\n",
       "  '-1:postag[:2]': ',',\n",
       "  '+1:word.lower()': 'age',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'age',\n",
       "  'word[-3:]': 'age',\n",
       "  'word[-2:]': 'ge',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'gestational',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'JJ',\n",
       "  '-1:postag[:2]': 'JJ',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': ',',\n",
       "  '+1:postag[:2]': ','},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': ',',\n",
       "  'postag[:2]': ',',\n",
       "  '-1:word.lower()': 'age',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'parity',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'parity',\n",
       "  'word[-3:]': 'ity',\n",
       "  'word[-2:]': 'ty',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': ',',\n",
       "  '-1:postag[:2]': ',',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': ',',\n",
       "  '+1:postag[:2]': ','},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': ',',\n",
       "  'postag[:2]': ',',\n",
       "  '-1:word.lower()': 'parity',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'and',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'CC',\n",
       "  '+1:postag[:2]': 'CC'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'and',\n",
       "  'word[-3:]': 'and',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'CC',\n",
       "  'postag[:2]': 'CC',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': ',',\n",
       "  '-1:postag[:2]': ',',\n",
       "  '+1:word.lower()': 'practice',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'practice',\n",
       "  'word[-3:]': 'ice',\n",
       "  'word[-2:]': 'ce',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'and',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'CC',\n",
       "  '-1:postag[:2]': 'CC',\n",
       "  '+1:word.lower()': 'group',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'group',\n",
       "  'word[-3:]': 'oup',\n",
       "  'word[-2:]': 'up',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'practice',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': '(',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': '(',\n",
       "  '+1:postag[:2]': '('},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': '(',\n",
       "  'word[-3:]': '(',\n",
       "  'word[-2:]': '(',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': '(',\n",
       "  'postag[:2]': '(',\n",
       "  '-1:word.lower()': 'group',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': 'to',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'TO',\n",
       "  '+1:postag[:2]': 'TO'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'to',\n",
       "  'word[-3:]': 'to',\n",
       "  'word[-2:]': 'to',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'TO',\n",
       "  'postag[:2]': 'TO',\n",
       "  '-1:word.lower()': '(',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': '(',\n",
       "  '-1:postag[:2]': '(',\n",
       "  '+1:word.lower()': 'reflect',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'VB',\n",
       "  '+1:postag[:2]': 'VB'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'reflect',\n",
       "  'word[-3:]': 'ect',\n",
       "  'word[-2:]': 'ct',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'VB',\n",
       "  'postag[:2]': 'VB',\n",
       "  '-1:word.lower()': 'to',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'TO',\n",
       "  '-1:postag[:2]': 'TO',\n",
       "  '+1:word.lower()': 'risk',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'risk',\n",
       "  'word[-3:]': 'isk',\n",
       "  'word[-2:]': 'sk',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'reflect',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'VB',\n",
       "  '-1:postag[:2]': 'VB',\n",
       "  '+1:word.lower()': 'status',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': 'NN',\n",
       "  '+1:postag[:2]': 'NN'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': 'status',\n",
       "  'word[-3:]': 'tus',\n",
       "  'word[-2:]': 'us',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': 'NN',\n",
       "  'postag[:2]': 'NN',\n",
       "  '-1:word.lower()': 'risk',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  '+1:word.lower()': ')',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.isdigit()': False,\n",
       "  '+1:postag': ')',\n",
       "  '+1:postag[:2]': ')'},\n",
       " {'bias': 0.75,\n",
       "  'word.lower()': ')',\n",
       "  'word[-3:]': ')',\n",
       "  'word[-2:]': ')',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'postag': ')',\n",
       "  'postag[:2]': ')',\n",
       "  '-1:word.lower()': 'status',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.isdigit()': False,\n",
       "  '-1:postag': 'NN',\n",
       "  '-1:postag[:2]': 'NN',\n",
       "  'EOS': True}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check Features for the 1st sentence in the train dataset\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----First Word - to check if BOS tag is present-----\n",
      "\n",
      "{'bias': 0.75, 'word.lower()': 'all', 'word[-3:]': 'All', 'word[-2:]': 'll', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'DT', 'postag[:2]': 'DT', 'BOS': True, '+1:word.lower()': 'live', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:word.isdigit()': False, '+1:postag': 'JJ', '+1:postag[:2]': 'JJ'} \n",
      "\n",
      "-----Last Word - to check if EOS tag is present-----\n",
      "\n",
      "{'bias': 0.75, 'word.lower()': ')', 'word[-3:]': ')', 'word[-2:]': ')', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': ')', 'postag[:2]': ')', '-1:word.lower()': 'status', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:word.isdigit()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', 'EOS': True} \n",
      "\n",
      "-----Any other word - to check that both EOS and BOS are not present-----\n",
      "\n",
      "{'bias': 0.75, 'word.lower()': 'births', 'word[-3:]': 'ths', 'word[-2:]': 'hs', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'NNS', 'postag[:2]': 'NN', '-1:word.lower()': 'live', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:word.isdigit()': False, '-1:postag': 'JJ', '-1:postag[:2]': 'JJ', '+1:word.lower()': '>', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:word.isdigit()': False, '+1:postag': 'VBP', '+1:postag[:2]': 'VB'}\n"
     ]
    }
   ],
   "source": [
    "# Lets check if EOS and BOS are correctly marked\n",
    "#BOS - 1st word of the 1st sentence\n",
    "print(\"-----First Word - to check if BOS tag is present-----\\n\")\n",
    "print(X_train[0][0],\"\\n\")\n",
    "#EOS - last word of the 1st sentence\n",
    "print(\"-----Last Word - to check if EOS tag is present-----\\n\")\n",
    "print(X_train[0][-1],\"\\n\")\n",
    "print(\"-----Any other word - to check that both EOS and BOS are not present-----\\n\")\n",
    "print(X_train[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are getting features and we can see the tags are also correctly added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets check the label for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are getting the feature value and the proper label as expected!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ERROR \n",
    " During the first instance of model fit, the fit failed. Upon further investigation it was found that there was a length mismatch between label and sentences. So before we go back to that we will try and find the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We had checked the data before POS tagging and it was alright. The assumption is that POS tagging could have caused \n",
    "## some error\n",
    "## We will manually check the mismatched sentences to see if we can find the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will check the length of sentences against their labels and find the sentences that have error\n",
    "def errorcheck(x,y): #--- x is the sentences and y is the labels\n",
    "    error=0 #-- Counter to see how many errors are there\n",
    "    print(\"-------Sentences with Mismatched Labels-------\")\n",
    "    for i in range(0,len(x)):\n",
    "        if(len(x[i])!=len(y[i])):\n",
    "            error=error+1\n",
    "            print(\"Index of sentence :\",i,\"---Length of Sentence :\",len(x[i]),\"---Length of Labels :\",len(y[i]))\n",
    "    if error==0:\n",
    "        print(\"No sentences with mismatched labels found\")\n",
    "    else:\n",
    "        print(\"Number of Sentences with mismatched labels are :\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Sentences with Mismatched Labels-------\n",
      "Index of sentence : 85 ---Length of Sentence : 38 ---Length of Labels : 32\n",
      "Index of sentence : 752 ---Length of Sentence : 56 ---Length of Labels : 55\n",
      "Index of sentence : 938 ---Length of Sentence : 13 ---Length of Labels : 12\n",
      "Index of sentence : 997 ---Length of Sentence : 15 ---Length of Labels : 14\n",
      "Number of Sentences with mismatched labels are : 4\n"
     ]
    }
   ],
   "source": [
    "errorcheck(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence===>\n",
      "\n",
      "RESULTS : The HLA effect is due to the class II DR2 haplotype , DRB1*1501-DQA1*0102-DRB1*0602 ; contributions to MS susceptibility from additional DRB1-DQB1 alleles or other HLA region loci were not observed \n",
      "\n",
      "Sentence after Tagging===>\n",
      "\n",
      "[('RESULTS', 'NN'), (':', ':'), ('The', 'DT'), ('HLA', 'NNP'), ('effect', 'NN'), ('is', 'VBZ'), ('due', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('class', 'NN'), ('II', 'NNP'), ('DR2', 'NNP'), ('haplotype', 'NN'), (',', ','), ('DRB1', 'NNP'), ('*', 'NNP'), ('1501-DQA1', 'JJ'), ('*', 'NNP'), ('0102-DRB1', 'CD'), ('*', 'NN'), ('0602', 'CD'), (';', ':'), ('contributions', 'NNS'), ('to', 'TO'), ('MS', 'NNP'), ('susceptibility', 'NN'), ('from', 'IN'), ('additional', 'JJ'), ('DRB1-DQB1', 'NNP'), ('alleles', 'NNS'), ('or', 'CC'), ('other', 'JJ'), ('HLA', 'NNP'), ('region', 'NN'), ('loci', 'NN'), ('were', 'VBD'), ('not', 'RB'), ('observed', 'VBN')] \n",
      "\n",
      "Label===>\n",
      "\n",
      "OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO \n",
      "\n",
      "Length of Label===>\n",
      "\n",
      "32 \n",
      "\n",
      "Length of Original Sentence===>\n",
      "\n",
      "32 \n",
      "\n",
      "Length of Tagged Sentence===>\n",
      "\n",
      "38\n",
      "----------------------------\n",
      "Original Sentence===>\n",
      "\n",
      "Lactate values obtained by continuous catheter operation ex vivo correlate well with those obtained by BIOSEN Med L. First subcutaneous implantation ( dog ) underlines the characteristics obtained ex vivo : after 30 min hydration the lactate catheter follows the lactate concentration measured ex vivo with samples from the leg vein by BIOSEN Med L. \n",
      "\n",
      "Sentence after Tagging===>\n",
      "\n",
      "[('Lactate', 'NNP'), ('values', 'NNS'), ('obtained', 'VBN'), ('by', 'IN'), ('continuous', 'JJ'), ('catheter', 'NN'), ('operation', 'NN'), ('ex', 'FW'), ('vivo', 'NN'), ('correlate', 'NN'), ('well', 'NN'), ('with', 'IN'), ('those', 'DT'), ('obtained', 'VBN'), ('by', 'IN'), ('BIOSEN', 'NNP'), ('Med', 'NNP'), ('L.', 'NNP'), ('First', 'NNP'), ('subcutaneous', 'JJ'), ('implantation', 'NN'), ('(', '('), ('dog', 'NN'), (')', ')'), ('underlines', 'VBZ'), ('the', 'DT'), ('characteristics', 'NNS'), ('obtained', 'VBD'), ('ex', 'JJ'), ('vivo', 'NN'), (':', ':'), ('after', 'IN'), ('30', 'CD'), ('min', 'NN'), ('hydration', 'NN'), ('the', 'DT'), ('lactate', 'NN'), ('catheter', 'NN'), ('follows', 'VBZ'), ('the', 'DT'), ('lactate', 'JJ'), ('concentration', 'NN'), ('measured', 'VBD'), ('ex', 'JJ'), ('vivo', 'NN'), ('with', 'IN'), ('samples', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('leg', 'NN'), ('vein', 'NN'), ('by', 'IN'), ('BIOSEN', 'NNP'), ('Med', 'NNP'), ('L', 'NNP'), ('.', '.')] \n",
      "\n",
      "Label===>\n",
      "\n",
      "OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO \n",
      "\n",
      "Length of Label===>\n",
      "\n",
      "55 \n",
      "\n",
      "Length of Original Sentence===>\n",
      "\n",
      "55 \n",
      "\n",
      "Length of Tagged Sentence===>\n",
      "\n",
      "56\n",
      "----------------------------\n",
      "Original Sentence===>\n",
      "\n",
      "Oviposition and its regulation in the polygynous society of Polistes gallicus L. \n",
      "\n",
      "Sentence after Tagging===>\n",
      "\n",
      "[('Oviposition', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('regulation', 'NN'), ('in', 'IN'), ('the', 'DT'), ('polygynous', 'JJ'), ('society', 'NN'), ('of', 'IN'), ('Polistes', 'NNP'), ('gallicus', 'NN'), ('L', 'NNP'), ('.', '.')] \n",
      "\n",
      "Label===>\n",
      "\n",
      "OOOOOOOOOOOO \n",
      "\n",
      "Length of Label===>\n",
      "\n",
      "12 \n",
      "\n",
      "Length of Original Sentence===>\n",
      "\n",
      "12 \n",
      "\n",
      "Length of Tagged Sentence===>\n",
      "\n",
      "13\n",
      "----------------------------\n",
      "Original Sentence===>\n",
      "\n",
      "A fine structural analysis of the epidermis of the earthworm , Lumbricus terrestris L. \n",
      "\n",
      "Sentence after Tagging===>\n",
      "\n",
      "[('A', 'DT'), ('fine', 'JJ'), ('structural', 'JJ'), ('analysis', 'NN'), ('of', 'IN'), ('the', 'DT'), ('epidermis', 'NN'), ('of', 'IN'), ('the', 'DT'), ('earthworm', 'NN'), (',', ','), ('Lumbricus', 'NNP'), ('terrestris', 'VBZ'), ('L', 'NNP'), ('.', '.')] \n",
      "\n",
      "Label===>\n",
      "\n",
      "OOOOOOOOOOOOOO \n",
      "\n",
      "Length of Label===>\n",
      "\n",
      "14 \n",
      "\n",
      "Length of Original Sentence===>\n",
      "\n",
      "14 \n",
      "\n",
      "Length of Tagged Sentence===>\n",
      "\n",
      "15\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Checking what went wrong\n",
    "baddata=[85,752,938,997] #---- > Index of sentences with errors obtained from error check\n",
    "for num in baddata:\n",
    "    print(\"Original Sentence===>\\n\")\n",
    "    print(train_sent[num],\"\\n\")\n",
    "    print(\"Sentence after Tagging===>\\n\")\n",
    "    print(train_sent_tagged[num],\"\\n\")\n",
    "    print(\"Label===>\\n\")\n",
    "    print(train_label[num],\"\\n\")\n",
    "    print(\"Length of Label===>\\n\")\n",
    "    print(len(train_label[num]),\"\\n\")\n",
    "    print(\"Length of Original Sentence===>\\n\")\n",
    "    print(len(train_sent[num].split()),\"\\n\")\n",
    "    print(\"Length of Tagged Sentence===>\\n\")\n",
    "    print(len(train_sent_tagged[num]))\n",
    "    print(\"----------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from above, these sentences have been tagged incorrectly example 'DRB1*1501-DQA1*0102-DRB1*0602' has\n",
    "# a single label in our data but pos tagging has broken it into multiple parts. Also L. which is considered a single \n",
    "# word as per label as been broken into 2\n",
    "\n",
    "## Given that the mismatched error is only on 4 sentences, we will remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2599\n",
      "2599\n"
     ]
    }
   ],
   "source": [
    "# Size of train before removal\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing mismatched sentences\n",
    "for i in range(85,997): #-- Because the indexes lie between these values\n",
    "        if(len(X_train[i])!=len(Y_train[i])): \n",
    "            del X_train[i]\n",
    "            del Y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2595\n",
      "2595\n"
     ]
    }
   ],
   "source": [
    "# Size of train after removal\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Sentences with Mismatched Labels-------\n",
      "No sentences with mismatched labels found\n"
     ]
    }
   ],
   "source": [
    "#Lets check to see if the error is eliminated in Train Data\n",
    "errorcheck(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Sentences with Mismatched Labels-------\n",
      "Index of sentence : 24 ---Length of Sentence : 27 ---Length of Labels : 25\n",
      "Number of Sentences with mismatched labels are : 1\n"
     ]
    }
   ],
   "source": [
    "# Lets check on test data too\n",
    "errorcheck(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have only 1 bad data, we will remove it\n",
    "# because we will use test data for building the dictionary later too, we are removing it from all list including \n",
    "# original test_sent and test_sent_tagged\n",
    "del X_test[24]\n",
    "del Y_test[24]\n",
    "del test_sent[24]\n",
    "del test_sent_tagged[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Sentences with Mismatched Labels-------\n",
      "No sentences with mismatched labels found\n"
     ]
    }
   ],
   "source": [
    "#Lets check to see if the error is eliminated in Test Data\n",
    "errorcheck(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Model Parameters\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1, # coefficient for L1 regularization\n",
    "    c2=0.1, # coefficient for L2 regularization\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=500)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for this model is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9255884062060182"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Prediction and Evaluation\n",
    "Y_pred = crf.predict(X_test) #-- Predict using the model above\n",
    "print(\"F1 Score for this model is:\")\n",
    "metrics.flat_f1_score(Y_test, Y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 121 candidates, totalling 363 fits\n",
      "best params: {'c1': 0.25, 'c2': 0}\n",
      "best CV score: 0.875601681374312\n"
     ]
    }
   ],
   "source": [
    "# GridSearch \n",
    "\n",
    "crf = sklearn_crfsuite.CRF(max_iterations=500, all_possible_transitions=True)\n",
    "params_space = {\n",
    "    \"c1\": [0,0.01,0.05,0.1, 0.25,0.5,0.6,0.7,0.8,0.9,1],\n",
    "    \"c2\": [0,0.01,0.05,0.1, 0.25,0.5,0.6,0.7,0.8,0.9,1]\n",
    "}\n",
    "\n",
    "# Using F1 score as the scorer\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted')\n",
    "\n",
    "# search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=crf,\n",
    "                           param_grid=params_space,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1, verbose=1,scoring=f1_scorer)\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print('best params:', grid_search.best_params_)\n",
    "print('best CV score:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for the best model from Gridsearch is(On Test Data):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9270757731516192"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_GCV = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"F1 Score for the best model from Gridsearch is(On Test Data):\")\n",
    "metrics.flat_f1_score(Y_test, Y_pred_GCV, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "best params: {'c1': 0.2382696683317979, 'c2': 0.005693003141979298}\n",
      "best CV score: 0.8759733013135981\n"
     ]
    }
   ],
   "source": [
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# Using F1 score as the scorer\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted')\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        n_iter=100, \n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, Y_train)\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for the best model from Randomsearch is(On Test Data):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9273203246324682"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_RCV = rs.best_estimator_.predict(X_test)\n",
    "print(\"F1 Score for the best model from Randomsearch is(On Test Data):\")\n",
    "metrics.flat_f1_score(Y_test, Y_pred_RCV, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got the highest F1 score from the randomsearch CV where L1 and L2 were :\n",
    "# 'c1': 0.2382696683317979, 'c2': 0.005693003141979298\n",
    "### we will use this as the final model###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best F1 Score on test data was from the Random Search Model and the F1 for that was 0.9273203246324682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the predicted labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Predicted Label 44\n",
      "Length of Original Label 44\n"
     ]
    }
   ],
   "source": [
    "# Checking for the length of the the labels for 1st sentence in the test data\n",
    "print(\"Length of Predicted Label\",len(Y_pred_RCV[0]))\n",
    "print(\"Length of Original Label\",len(test_label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: He needed mechanical ventilation for severe hypoxemia , but recovered with glucocorticoid pulse therapy\n",
      "Sentence: 14\n",
      "Orig Labels: 14\n",
      "Pred Labels: 14\n",
      "Orig Labels: ['O', 'O', 'T', 'T', 'O', 'D', 'D', 'O', 'O', 'O', 'O', 'T', 'T', 'T']\n",
      "Pred Labels: ['O', 'O', 'T', 'T', 'O', 'D', 'D', 'O', 'O', 'O', 'O', 'T', 'T', 'T']\n"
     ]
    }
   ],
   "source": [
    "## Checking a random sentence from the test data to see the predicted and actual label\n",
    "id = 935 ##-- Random index to check\n",
    "print(\"Sentence:\",test_sent[id])\n",
    "print(\"Sentence:\",len(test_sent[id].split()))\n",
    "print(\"Orig Labels:\", len(Y_test[id]))\n",
    "print(\"Pred Labels:\", len(Y_pred_RCV[id]))\n",
    "print(\"Orig Labels:\", (Y_test[id]))\n",
    "print(\"Pred Labels:\", (Y_pred_RCV[id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks like the model predicted labels are vey close to the actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the logic to get all the predicted treatments (T) labels corresponding to each disease (D) label in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "How -\n",
    "Every Sentence was scanned to look for D and T tag and if both D & T tags were present that the D tags were concatenated to form the disease name and T tags were concatenated to find the treatment. These were than added as key-value pairs to a dictionary\n",
    "Code ensured that if a disease was already present, than the just the treatment was added to existing values\n",
    "\n",
    "### Qualifier\n",
    "\n",
    "The disease names were also concatenated with a qualifier.\n",
    "The choice was between prefix or an adjective. Prefix would have added a lot of unnecessary information so adjective was used.\n",
    "This qualifier was added only if the adjective was found before the first D tag in a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat={} #--- Dictionary to store the Disease-treatment as key-value pair\n",
    "for i in range(0,len(Y_pred_RCV)):\n",
    "    if \"D\" and \"T\" in Y_pred_RCV[i]:\n",
    "        words=test_sent[i].lower().split()\n",
    "        disease=\"\"\n",
    "        treatment=\"\"\n",
    "        for j in range(0,len(Y_pred_RCV[i])):\n",
    "            if((Y_pred_RCV[i][j])==\"D\"):\n",
    "                if j==0:\n",
    "                    disease=disease+\" \"+words[j]\n",
    "                else:\n",
    "\n",
    "                    if(test_sent_tagged[i][j-1][1]==\"JJ\" and (Y_pred_RCV[i][j-1])!=\"D\" ):\n",
    "                        disease=disease+\" \"+test_sent_tagged[i][j-1][0]+\" \"+words[j]\n",
    "                    else:\n",
    "\n",
    "                        disease=disease+\" \"+words[j]\n",
    "            if((Y_pred_RCV[i][j])==\"T\"):\n",
    "                treatment=treatment+\" \"+words[j]\n",
    "        if(len(treatment)!=0 and len(disease)!=0):\n",
    "            disease=disease.strip()\n",
    "            treatment=treatment.strip()\n",
    "            if disease in treat.keys():\n",
    "\n",
    "                treat[disease].add(treatment)\n",
    "            else:\n",
    "\n",
    "                treat[disease]={treatment}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macrosomic infants in gestational diabetes cases ====>>>>>> {'good glycemic control'}\n",
      "hereditary retinoblastoma ====>>>>>> {'radiotherapy'}\n",
      "epilepsy adhd ====>>>>>> {'methylphenidate'}\n",
      "unstable angina or non-q-wave myocardial infarction ====>>>>>> {'roxithromycin'}\n",
      "coronary-artery disease ====>>>>>> {'antichlamydial antibiotics'}\n",
      "primary pulmonary hypertension ( pph ) ====>>>>>> {'fenfluramines'}\n",
      "essential hypertension ====>>>>>> {'moxonidine'}\n",
      "cellulitis ====>>>>>> {'g-csf therapy intravenous antibiotic treatment'}\n",
      "foot infection in diabetic patients ====>>>>>> {'g-csf treatment'}\n",
      "stroke hemorrhagic stroke ====>>>>>> {'double-bolus alteplase accelerated infusion of alteplase ( p=0.24'}\n",
      "cardiac disease ====>>>>>> {'fenfluramine-phentermine'}\n",
      "rheumatoid arthritis ====>>>>>> {'arthrodesis'}\n",
      "early parkinson 's disease ====>>>>>> {'ropinirole monotherapy'}\n",
      "sore throat ====>>>>>> {'antibiotics'}\n",
      "crohn 's disease ====>>>>>> {'steroids'}\n",
      "stress urinary incontinence ====>>>>>> {'surgery', 'surgical procedures'}\n",
      "female stress urinary incontinence ====>>>>>> {'surgical treatment'}\n",
      "corpal gastritis ====>>>>>> {'gastric acid secretion'}\n",
      "preeclampsia ( proteinuric hypertension ) ====>>>>>> {'intrauterine insemination with donor sperm intrauterine insemination'}\n",
      "mild preeclampsia preeclampsia ====>>>>>> {'partner insemination program donor insemination program'}\n",
      "hyperammonemia cancer ====>>>>>> {'organ transplantation and chemotherapy'}\n",
      "major pulmonary embolism ====>>>>>> {'thrombolytic treatment'}\n",
      "malignant pleural mesothelioma ====>>>>>> {'thoracotomy , radiotherapy , and chemotherapy'}\n",
      "hearing and receptive language disorders ====>>>>>> {'neonatal ecmo'}\n",
      "mediastinal teratomas ====>>>>>> {'chemotherapy'}\n",
      "non-obstructive azoospermia ====>>>>>> {'testicular fine needle aspiration ( tefna open biopsy and testicular sperm extraction ( tese )', 'tefna'}\n",
      "testicular bleeding ====>>>>>> {'fine needle aspiration tese'}\n",
      "hyperlipidemias ====>>>>>> {'statins'}\n",
      "acute migraine treatment ====>>>>>> {'sumatriptan'}\n",
      "duodenal ulcer ====>>>>>> {'measured subtotal gastrectomy'}\n",
      "colorectal cancer ====>>>>>> {'uft', 'elective surgery', 'leucovorin or cisplatin'}\n",
      "gastrointestinal tumours ====>>>>>> {'elective surgery'}\n",
      "primary pulmonary hypertension ====>>>>>> {'fenfluramine'}\n",
      "restenosis ====>>>>>> {'coronary angioplasty'}\n",
      "oral crohn 's disease ====>>>>>> {'thalidomide'}\n",
      "progressive sensorineural hearing loss ====>>>>>> {'elevated blood lipids'}\n",
      "bronchial asthma ====>>>>>> {'non-steroidal anti-inflammatory therapy'}\n",
      "parkinson 's disease ====>>>>>> {'microelectrode-guided posteroventral pallidotomy'}\n",
      "hepatitis virus strain 3 infection ====>>>>>> {'fulminant hepatic failure'}\n",
      "alopecia ====>>>>>> {'tacrolimus therapy in renal transplantation'}\n",
      "hypertension ====>>>>>> {'which calcium antagonists'}\n",
      "soft tissue sarcomas ====>>>>>> {'radiotherapy'}\n",
      "eisenmenger 's syndrome ====>>>>>> {'laparoscopic cholecystectomy'}\n",
      "advanced esophageal cancer ====>>>>>> {'adjuvant chemoradiotherapy with cddp , 5-fu , and vp-16'}\n",
      "breast cancer ====>>>>>> {'undergone subcutaneous mastectomy', 'hormone replacement therapy'}\n",
      "severe psoriasis ====>>>>>> {'systemic treatments'}\n",
      "non-seminomatous germ-cell tumors ====>>>>>> {'chemotherapy'}\n",
      "lung metastasis ====>>>>>> {'mdp-lys treatment'}\n",
      "malignant tumors non-small cell lung cancer ====>>>>>> {'surgery'}\n",
      "recurrent cervical adenocarcinoma ====>>>>>> {'weekly paclitaxel'}\n",
      "colds colds ====>>>>>> {'antibiotics antibiotics'}\n",
      "abdominal pain ====>>>>>> {'thoracic paravertebral block ( tpvb )'}\n",
      "fungal infections ====>>>>>> {'triazole antifungal agents , fluconazole and itraconazole'}\n",
      "inflammatory skin diseases ====>>>>>> {'topical corticosteroids'}\n",
      "atopic dermatitism ====>>>>>> {'tacrolimus and ascomycin derivatives'}\n",
      "cancer leukemia group b unresectable stage iii nsclc ====>>>>>> {'sequential chemotherapy followed by radiation'}\n",
      "nsclc nsclc sclc ====>>>>>> {'surgical treatment radiotherapy radiotherapy'}\n",
      "nsclc ====>>>>>> {'sequential chemotherapy', 'platinum-based chemotherapy'}\n",
      "bos ====>>>>>> {'therapy with extracorporeal photopheresis'}\n",
      "early bos ====>>>>>> {'extracorporeal photopheresis promising therapy'}\n",
      "locally advanced non-small-cell lung cancer ( la-nsclc ) ====>>>>>> {'chemotherapy and radiotherapy )'}\n",
      "radiation-induced myelopathy ====>>>>>> {'heparin and enoxaparin'}\n",
      "limited stage small cell lung cancer ====>>>>>> {'vip combination chemotherapy'}\n",
      "malignant pleural effusions from nsclc ====>>>>>> {'systemic chemotherapy'}\n",
      "small-cell lung cancer ====>>>>>> {'combination chemotherapy', 'paclitaxel plus carboplatin ( pc ) vinorelbine plus cisplatin ( vc )', 'chemotherapy'}\n",
      "intraluminal early-stage cancer ====>>>>>> {'photodynamic therapy , nd-yag laser and electrocautery'}\n",
      "pathologic stage i non-small cell lung cancer ====>>>>>> {'postoperative oral administration of uft , a 5-fluorouracil derivative chemotherapeutic agent'}\n",
      "supraclavicular node metastases in nsclc ====>>>>>> {'chemoradiotherapy'}\n",
      "non-small-cell-lung-cancer ( nsclc ) ====>>>>>> {'cisplatin and radiotherapy'}\n",
      "lung carcinoma ====>>>>>> {'curative therapy', 'videothoracoscopic lobectomy or partial resection of the lung open thoracotomy'}\n",
      "large lesions ====>>>>>> {'gks'}\n",
      "single non-sclc melanoma ovarian carcinoma brain metastasis ====>>>>>> {'surgical resection'}\n",
      "colorectal metastases ====>>>>>> {'therapeutic vats metastasectomy'}\n",
      "advanced nsclc ====>>>>>> {'combination chemotherapy of cisplatin , ifosfamide and irinotecan with rhg-csf support'}\n",
      "metastatic colorectal cancer ====>>>>>> {'intravenous oxaliplatin'}\n",
      "platinum-pretreated ovarian cancer ====>>>>>> {'oxaliplatin paclitaxel'}\n",
      "non-hodgkin 's lymphoma breast cancer mesothelioma and non-small cell lung cancer ====>>>>>> {'oxaliplatin'}\n",
      "other cancers platinum-pretreated ovarian cancer ====>>>>>> {'oxaliplatin'}\n",
      "primary tumor bronchogenic carcinoma ====>>>>>> {'resection'}\n",
      "non-small cell lung cancer prostate cancer ====>>>>>> {'paclitaxel and carboplatin'}\n",
      "primary lung cancer adenocarcinoma ( ad ) squamous cell carcinoma ====>>>>>> {'resection'}\n",
      "stage iii nsclc ====>>>>>> {'chemotherapy surgery or definitive irradiation'}\n",
      "symptomatic metastases ====>>>>>> {'radiotherapy'}\n",
      "primary cancer ====>>>>>> {'adjuvant radiation therapy'}\n",
      "sclc ====>>>>>> {'platinum dose ( cisplatin plus carboplatin ) in combination chemotherapy combination therapy with carboplatin'}\n",
      "untreated small cell lung cancer ( sclc ) untreated sclc ====>>>>>> {'chemotherapy'}\n",
      "neutropenia cancer ====>>>>>> {'chemotherapy'}\n",
      "head and neck cancer =2 xerostomia ====>>>>>> {'irradiation therapy'}\n",
      "psoriasis ====>>>>>> {'topical therapy', 'active vitamin d3 analogue , 1 alpha'}\n",
      "third nerve palsy ====>>>>>> {'lateral rectus surgery'}\n",
      "disseminated malignant melanoma ====>>>>>> {'leukocyte a recombinant interferon ( rifn-alpha'}\n",
      "advanced stage ( tnm iib-ivb ) mycosis fungoides ====>>>>>> {'combination chemotherapy program consisting of bleomycin and methotrexate weekly'}\n",
      "ventricular tachycardia ====>>>>>> {'surgical therapy'}\n",
      "colds ====>>>>>> {'therapeutic supplementation vitamin c'}\n",
      "cholestasis ====>>>>>> {'inchinko-to ( tj-135 ) is a herbal medicine consisting'}\n",
      "severe acute hepatitis accompanying cholestasis autoimmune hepatitis ====>>>>>> {'tj-135'}\n",
      "syringomyelia spinal adhesive arachnoiditis ====>>>>>> {'surgical management'}\n",
      "bronchiectasis ====>>>>>> {'antibiotics and surgery'}\n",
      "biliary colic symptoms biliary dyskinesia ====>>>>>> {'cholecystectomy'}\n",
      "biliary dyskinesia ====>>>>>> {'cholecystectomy'}\n",
      "paranasal sinuses associated with the common cold ====>>>>>> {'pseudoephedrine plus acetaminophen'}\n",
      "inflammation ====>>>>>> {'video-assisted thoracoscopic surgery'}\n",
      "viral infections ====>>>>>> {'interferon'}\n",
      "acute nasopharyngitis ( anp ) ====>>>>>> {'antibiotic treatment'}\n",
      "head lice ====>>>>>> {'permethrin'}\n",
      "common cold ====>>>>>> {'macrolide antibiotics'}\n",
      "rhinovirus colds infection ====>>>>>> {'clarithromycin'}\n",
      "multidrug resistant tb ====>>>>>> {'therapeutic pneumothorax ( tp )'}\n",
      "influenza breast cancer ====>>>>>> {'vaccination'}\n",
      "mouse-adopted strain of influenza a2 ( h2n2 ) virus ====>>>>>> {'gingyo-san'}\n",
      "infection ====>>>>>> {'combination of omeprazole , amoxicillin , and clarithromycin'}\n",
      "carcinoma ====>>>>>> {'drainage methods esophagectomy'}\n",
      "persistent asthma ====>>>>>> {'contemporary asthma management guidelines list inhaled corticosteroids'}\n",
      "asthma ====>>>>>> {'fluticasone propionate'}\n",
      "chronic hepatitis c ====>>>>>> {'combination therapy interferon-alpha ( ifn alpha ) plus ribavirin'}\n",
      "hepatitis c viremia ====>>>>>> {'ribavirin combination therapy'}\n",
      "cbd stones ====>>>>>> {'one-time surgical exploration'}\n",
      "duodenogastric reflux ====>>>>>> {'cholecystectomy'}\n",
      "severe hypoxemia ====>>>>>> {'mechanical ventilation glucocorticoid pulse therapy'}\n",
      "aom drug-resistant s. pneumoniae ====>>>>>> {'amoxicillin remains the antibiotic of choice'}\n",
      "bacterial meningitis ====>>>>>> {'vaccines'}\n",
      "primary sclerosing cholangitis ( psc ) ====>>>>>> {'oral budesonide'}\n",
      "age-related macular degeneration ( amd ) ====>>>>>> {'external beam radiation therapy'}\n",
      "acute myocardial infarction ====>>>>>> {'thrombolytic treatment', 'thrombolytic therapy'}\n",
      "depression hyponatremia ====>>>>>> {'venlafaxine'}\n",
      "ischemic heart disease ====>>>>>> {'aortocoronary bypass grafting'}\n",
      "cancers autologous cancer primary cancer metastatic load ====>>>>>> {'immunotherapy'}\n",
      "s.c. tumors peritoneal tumors ====>>>>>> {'subcutaneous injection of irradiated llc-il2 did not'}\n",
      "acute occlusion of the middle cerebral artery ====>>>>>> {'thrombolytic therapy'}\n",
      "benign prostatic hyperplasia ( bph ) ====>>>>>> {'conservative pharmacological treatment'}\n",
      "autoimmune diseases ====>>>>>> {'high-dose intravenous immunoglobulin ( hdivig )'}\n",
      "cancer ====>>>>>> {'matrix metalloproteinase inhibitors', 'oral drugs chemotherapy'}\n",
      "large-bowel cancer ====>>>>>> {'oral uft'}\n",
      "phaeochromocytoma ====>>>>>> {'adrenalectomy'}\n",
      "malignant melanoma ====>>>>>> {'interferon alfa-2a'}\n",
      "advanced renal cell carcinoma ====>>>>>> {'various interferon alpha preparations interferon alfa-n1 , interferon alfa-2a , and interferon alfa-2b'}\n",
      "hairy cell leukemia infection ====>>>>>> {'antileukemic therapy'}\n",
      "low-grade non-hodgkin 's lymphoma ====>>>>>> {'recombinant and natural forms of interferon alpha'}\n",
      "low-grade non-hodgkin 's lymphomas ====>>>>>> {'interferon interferon'}\n",
      "partial seizures ====>>>>>> {'lamotrigine monotherapy', 'lamotrigine ( ltg )'}\n",
      "esophageal achalasia ====>>>>>> {'botulinum toxin injection , pneumatic dilation , and laparoscopic myotomy'}\n",
      "irritable bowel syndrome ====>>>>>> {'chinese herbal medicine'}\n",
      "proximal hypospadias ====>>>>>> {'tubularized incised plate hypospadias repair'}\n",
      "prostate cancer ====>>>>>> {'radical prostatectomy and iodine 125 interstitial radiotherapy'}\n",
      "tumors ====>>>>>> {'immunotherapy'}\n",
      "sickle cell disease ====>>>>>> {'hydroxyurea'}\n",
      "stroke ====>>>>>> {'statins'}\n",
      "mitomycin-resistant bladder cancer ====>>>>>> {'photodynamic therapy in combination with mitomycin c'}\n",
      "b16 melanoma ====>>>>>> {'adenosine triphosphate and treatment with buthionine sulfoximine'}\n",
      "primary uveal melanoma ====>>>>>> {'transpupillary thermotherapy'}\n",
      "advanced rectal cancer ====>>>>>> {'nerve-sparing surgery'}\n",
      "spontaneous pneumothorax ====>>>>>> {'thoracoscopic surgery'}\n",
      "acute cerebral ischemia ====>>>>>> {'antiplatelet therapy'}\n",
      "renal cell carcinoma ====>>>>>> {'interferon treatment'}\n",
      "barrett 's esophagus ====>>>>>> {'acid suppression therapy'}\n",
      "autoimmune hemolytic anemia ====>>>>>> {'heparin'}\n",
      "severe hypertension ====>>>>>> {'methyldopa'}\n",
      "epithelial ovarian cancer ====>>>>>> {'high-dose chemotherapy with autologous stem-cell support', 'chemotherapy'}\n",
      "stroke brain hemorrhage atrial fibrillation ====>>>>>> {'antiplatelet therapy'}\n",
      "noninsulin-dependent diabetes mellitus ====>>>>>> {'troglitazone'}\n",
      "lymphoma ====>>>>>> {'paclitaxel'}\n",
      "renovascular hypertension ====>>>>>> {'percutaneous transluminal angioplasty'}\n",
      "pulmonary hypertension ====>>>>>> {'double lung transplantation'}\n",
      "multiple sclerosis ====>>>>>> {'intravenous immunoglobulin treatment', 'interferon beta treatment'}\n",
      "acoustic neuroma ====>>>>>> {'stereotactic radiosurgery'}\n",
      "cerebral palsy ====>>>>>> {'hyperbaric oxygen therapy'}\n",
      "postvitrectomy diabetic vitreous hemorrhage ====>>>>>> {'peripheral retinal cryotherapy'}\n",
      "acute carbon monoxide poisoning ====>>>>>> {'hyperbaric or normobaric oxygen'}\n",
      "novel hepatitis b ====>>>>>> {'vaccine'}\n",
      "pertussis ====>>>>>> {'vaccines'}\n",
      "migraine ====>>>>>> {'sumatriptan'}\n",
      "temporomandibular joint arthropathy ====>>>>>> {'arthroscopic treatment'}\n",
      "acute colonic pseudo-obstruction ====>>>>>> {'neostigmine'}\n",
      "perioperative mortality and myocardial infarction ====>>>>>> {'vascular surgery'}\n",
      "severe secondary peritonitis ====>>>>>> {'surgical management'}\n",
      "hepatic metastases from colorectal cancer ====>>>>>> {'hepatic arterial infusion of chemotherapy'}\n",
      "stable asthma ====>>>>>> {'salbutamol'}\n",
      "chronic renal failure ====>>>>>> {'epoetin'}\n",
      "responsive multiple myeloma ====>>>>>> {'chemoradiotherapy with autologous stem-cell support'}\n"
     ]
    }
   ],
   "source": [
    "for key in treat:\n",
    "    print(key,\"====>>>>>>\",treat[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find treatment if disease is present in the dictionary\n",
    "def treatmentfor(disease):\n",
    "    try:\n",
    "        disease=disease.lower()\n",
    "        alltreat=treat[disease]\n",
    "        print(\"The treatment(s) for\",disease,\"is/are\\n\")\n",
    "        for treatments in alltreat:\n",
    "            print(treatments)\n",
    "    except:\n",
    "        print(\"We currently do not have any treatment for\",disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets see if we can predict the treatment by using the dictionary\n",
    "# We will do this by looking up the value of the key (which here is the disease we want to find the treatment for)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing treatmentfor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The treatment(s) for hereditary retinoblastoma is/are\n",
      "\n",
      "radiotherapy\n"
     ]
    }
   ],
   "source": [
    "#Test Case 1- Single Treatment in dictionary\n",
    "# Lets predict the treatment for hereditary retinoblastoma\n",
    "treatmentfor(\"hereditary retinoblastoma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The treatment(s) for multiple sclerosis is/are\n",
      "\n",
      "intravenous immunoglobulin treatment\n",
      "interferon beta treatment\n"
     ]
    }
   ],
   "source": [
    "#Test Case 2 - Multiple treatment in dictionary\n",
    "# Lets predict the treatment for multiple sclerosis\n",
    "treatmentfor(\"multiple sclerosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We currently do not have any treatment for covid\n"
     ]
    }
   ],
   "source": [
    "#Test Case 3 - Disease not in dictionary\n",
    "# Lets predict the treatment for multiple sclerosis\n",
    "treatmentfor(\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The treatment(s) for hereditary retinoblastoma is/are\n",
      "\n",
      "radiotherapy\n"
     ]
    }
   ],
   "source": [
    "# Once again lets us print put the treatment for hereditary retinoblastoma\n",
    "treatmentfor(\"hereditary retinoblastoma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
